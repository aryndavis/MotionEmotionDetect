{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b> Final Project: Emotional Body Language in Golf <br>\n",
    "COMS 4735 <br>\n",
    "Aryn Davis; ald2223 </b> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction:</h2>\n",
    "<br>\n",
    "I have been playing golf for the last 11 years of my life, and one thing that every instructor tells young golfers to do is manage their emotions and reactions while on the course. Even though you can sometimes see professional players explode on the course, they are usually very composed and show little emotion regarding a single shot. One of my favorite golfers since forever has been Tiger Woods, and he is known for being fairly composed while on the course. I will be creating a system to classify his resulting shot as either ‘good’ (fairway or green), ‘bad’ (1st or 2nd cut of rough, sand bunkers), or ‘horrible’ (in the trees, in the water, hazards, etc.), based on his body language, I believe that this is a valid visual interface. I don’t believe that this will be building upon prior work, but it will be similar in classification to the first assignment where we look at certain aspects of the pictures to decide what the picture represents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prior Works:</h2>\n",
    " <br>\n",
    "In doing research on the intersection between computer vision and golf, I found that detecting emotion from body language is a fairly difficult since there isn't much body language-related content out there (Noroozi et al., 2018). This study in particular found that while people can easily determine what is happening geometrically, it is hard to go deeper into that and figure out the exact emotions being portrayed. They suggest that emotional body language analysis is way farther behind emotional facial expression analysis, due to the fact that there is no largely available datasets of body gestures classified with emotions to develop any models of. For this project, I agree that facial expression analysis would be more beneficial, and hopefully since I am using crude motion detection for my clips, I will not need as deep of analysis in the body movement to draw conclusions. \n",
    " <br><br>\n",
    "In another study, centered on video analysis of putting, they found that it is possible to detect and idetify signature aspects of the putt swing that are unique to each player (Couceiro et al., 2013). While I won't be directly working with the golf swing, this idea of identifying aspects of the swing could be useful in future iterations of this project. By being able to identify when a golfer is in the follow through of their swing, it could make the data for a project like this more constrained, as you can start data collection the minute the follow through has started, so that you know you are getting body movement related to perception of the shot and not accidentally collecting noise from other parts of the swing.\n",
    "<br><br>\n",
    "A third study I found, human pose estimations from static images prove a significant challenge due to the high degrees of freedom in the human body that shift meaning and the lack of efficiency in current algorithms (Gong et al., 2016). At the end of this paper, they also mention the difficulty of estimating human poses when some body parts are hidden from the image. I found the same to be slightly difficult when crowdsourcing predictions from my family. Since most golf footage is taken from behind or to the right of the player, some of their limbs are almost always concealed throughout the swing. In the follow through stance with a camera angle from the right side, when players are facing the way their ball was hit, you cannot see what their face or hands are doing in most cases. Because of this, when asking my family to classify a clip based on the body language, there were instances where the concealed view meant that my family couldn't come to the same conclusion of what emotion was being portrayed because they couldn't see the pose. Since that article points to algorithms being able to detect pose even with partial view of the body, I see that as an open challenge of great difficulty since humans have difficulty with the same task, making it even harder to establish and annotate the appropriate dataset to solve this challenge. \n",
    "<br><br> \n",
    "In this study, on pose estimation in golf, they found an accurate system to determine what stage of the swing a golfer is at with a fron facing camera (Park et al., 2017). This system is also 3D and can be used for identifying improper movement and posture during the swing. However, the detection functionality can be used in a similar way in this project as the Courceiro study, where propoerly identifying the correct position of the golf swing in data can make swing data much more constrained, even in tournament play where the emotional detection can be analyzed in a real world situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data: </h2>\n",
    "<br>\n",
    "Check this Google drive link in case videos got ruptured in zipping process: https://drive.google.com/drive/folders/1HyAJhHmEhKehpM5Gw3qajVOf-264nPKi?usp=sharing. \n",
    "<br>I got different shot videos and named them as such, representing tee shots, long iron approach shots (>140 yards from the pin), short game (bunkers, chips, or pitch shots), and putts. Each shot has a different type of follow-through routine, so I thought it would be helpful to look at each set of shots differently. This link also includes some examples of the processed videos (I couldn't get all of them because the OpenCV VideoWriter was giving me issues, so I had to screen record all of them which was difficult, but I got enough to show the gist), including the binarized video of different movements, the color video with rectangles around each relevant moving part, and the gray-scale video of the highlighted changes between each video frame.\n",
    "<br>\n",
    "<br>\n",
    "In general, the data was very hard to collect. I screen recorded my clips from videos posted online from tournaments. Many tournaments focus more on the ball trajectory rather than the golfer, so I found many clips that didn't show Tiger's full response to his shot and would instead record how the ball was landing. Also, with the many camera angles provided in golf footage, often times after showing a full body swing, the cameras would switch to a close up of his face. These issues explains why some of my videos are just a second long, since I had to get only the body language before the camera switched to something else. I tried to get the most complete data that I could, and ended up getting the majority of the videos from one tournament, the 2018 PGA Championship because they had the most clear, most Tiger-focused footage that I could find. That tournament also had a video on YouTube of every single shot he had in the tournament, which was super helpful for getting data (this video: https://www.youtube.com/watch?v=ExiuB6wicus). I got a few other clips from other sources in there, but not too many. Overall, I got around 25 videos, at least 5 in each category and made sure there was at least one video representing the 3 classes I assigned.  \n",
    "<br> \n",
    "<br>\n",
    "Another aspect of getting data was cutting out the noise, at first I had the videos show Tiger's full swing, but then I figured that that was irrelevant to this project, since he only reacts to the shot after he has followed through and hit the ball, so I don't need all of the motion to potentially ruin my motion estimator. Some clubs are bigger than others, so including the swing will also have introduced more noise since the same swing but with more clubhead will make it seem like there was more movement for him, when that is not necessarily the case. Also, when Tiger plays in tournaments, he is surrounded by crowds of people who are very active spectators. In clips where the crowd was fairly close to him, when he hit a shot people would instantly move to try to see when it will land. This also introduced a fair bit of noise, which I included in the code to ignore, but I also had to go through and make the window of my recordings smaller. In most cases, I had to take multiple screen recordings of the same shot with different clip sizes and times in order to get the cleanest version of his body language that I could. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Code: </h2>\n",
    "<br>\n",
    "For my overarching plan of attack for this project, I first thought that the time he spent starting at the ball would be a huge indicator of how he felt about the shot, but after looking at the values from my motion detector, I found that it was the amount of movement after the shot that was the most valuable. And with each shot, the idea of 'goodness' and how long he would look at a shot varied. \n",
    "<br><br>\n",
    "So I found a tutorial online to do motion detection of a video in OpenCV here: https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/ (I think I have permission to use this since it is an tutorial for detection on an open source package.) And from there I modified it for video rather than webcam streaming. For each frame in the video, the algorithm will detect contours by looking at the change in pixels between the first frame and the current frame, binarizes on that difference and then loops through all of the contours present to place rectangles around them. I keep track of the total difference in pixels between the first frame and the current frame and the amount of pixels that differ between the current frame and the previous frame. With these two metrics, I want to see both how much movement overall took place, but also how little movement happened between frames. I want both of these because from looking at the videos, my initial assumptions about how he interprets full swing shots were somewhat right. He tends to move to a resting position and then stare on bad shots because he is trying to keep track of where they land, with good shot, he tends to move quickly out of his swing because he knows it'll land where he wants, and with horrible shots he tends to stare down with some additional movements to express frustration. The only category that didn't seem to fit this was the longiron shots. I suspect this because with these shots, he wants to get as close to the hole as possible and he is usually on the fairway or a first cut of the rough that would make this accessible (as opposed to short game, where he could be in a bunker or deep rough that would lessen his expectations slightly), and so with these he tends to do the opposite, quickly move from horrible shots, stare + move at bad shots, and stare down good shots. And on putts, it follows somewhat similar patterns, with bad putts (putts where he barely missed the hole) he stares it down in almost disbelief for a touch longer, while on good putts he watches it go into the hole and celebrates with a fist pump, and on horrible putts (putts where he missed the hole by over 1 ft) he tends to move quickly to either mark it or putt it in. \n",
    "<br><br>\n",
    "From these new observations, I can then tell if the total movement or minimum amount of movement is a good measure for a group of shots. For putts, long irons, and tee shots, it was better to use minimum movement, since it provided a better binary to whether he was staring at a shot or moving toward it/making frustrated gestures, which is easy to distinguish along since stare down would result in a smaller minimum than moving hectic. For short game, I found it more beneficial to use total movement, but I am not quite sure why. I discovered it during tuning the system when testing the different metrics. If I had to guess, I would say it was because short game has a smaller ranges of movement because you are so close to the hole and so the body language cues are smaller because you have to pay attention to how the green is sloping. But that's not a guarantee. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee1 good\n",
      "tee2 bad\n",
      "tee3 horrible\n",
      "tee4 horrible\n",
      "tee5 good\n",
      "tee6 bad\n",
      "tee7 good\n",
      "longiron1 good\n",
      "longiron2 horrible\n",
      "longiron3 bad\n",
      "longiron4 good\n",
      "longiron5 horrible\n",
      "longiron6 bad\n",
      "short1 good\n",
      "short2 horrible\n",
      "short3 bad\n",
      "short4 bad\n",
      "short5 good\n",
      "short6 bad\n",
      "short7 horrible\n",
      "putt1 horrible\n",
      "putt2 good\n",
      "putt3 good\n",
      "putt4 good\n",
      "putt5 bad\n",
      "\n",
      "{'tee1': 'good', 'tee2': 'bad', 'tee3': 'horrible', 'tee4': 'horrible', 'tee5': 'good', 'tee6': 'bad', 'tee7': 'good', 'longiron1': 'good', 'longiron2': 'horrible', 'longiron3': 'bad', 'longiron4': 'good', 'longiron5': 'horrible', 'longiron6': 'bad', 'short1': 'good', 'short2': 'horrible', 'short3': 'bad', 'short4': 'bad', 'short5': 'good', 'short6': 'bad', 'short7': 'horrible', 'putt1': 'horrible', 'putt2': 'good', 'putt3': 'good', 'putt4': 'good', 'putt5': 'bad'}\n"
     ]
    }
   ],
   "source": [
    "#displaying my assigned truth values!!!!\n",
    "\n",
    "#open my truths file for reading\n",
    "p = open(\"truth.txt\", \"r\")\n",
    "truths = {}\n",
    "if p.mode == 'r':\n",
    "    #take each line in file and read it \n",
    "    #store them in a dictionary so that i can use them for testing later\n",
    "    for line in p:\n",
    "        (key, val) = line.split()\n",
    "        print(key,val)\n",
    "        truths[key] = val\n",
    "print()\n",
    "print(truths)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee1 good good bad\n",
      "tee2 good good good\n",
      "tee3 bad horrible horrible\n",
      "tee4 horrible horrible horrible\n",
      "tee5 good good good\n",
      "tee6 horrible horrible bad\n",
      "tee7 good good good\n",
      "longiron1 bad good bad\n",
      "longiron2 bad bad good\n",
      "longiron3 horrible horrible bad\n",
      "longiron4 good good good\n",
      "longiron5 horrible horrible bad\n",
      "longiron6 horrible horrible horrible\n",
      "short1 good good good\n",
      "short2 bad horrible horrible\n",
      "short3 good bad good\n",
      "short4 horrible horrible bad\n",
      "short5 good bad good\n",
      "short6 bad good good\n",
      "short7 horrible horrible bad\n",
      "putt1 bad bad good\n",
      "putt2 good good good\n",
      "putt3 good good good\n",
      "putt4 bad good good\n",
      "putt5 bad bad horrible\n",
      "\n",
      "{'tee1': ['good', 'good', 'bad'], 'tee2': ['good', 'good', 'good'], 'tee3': ['bad', 'horrible', 'horrible'], 'tee4': ['horrible', 'horrible', 'horrible'], 'tee5': ['good', 'good', 'good'], 'tee6': ['horrible', 'horrible', 'bad'], 'tee7': ['good', 'good', 'good'], 'longiron1': ['bad', 'good', 'bad'], 'longiron2': ['bad', 'bad', 'good'], 'longiron3': ['horrible', 'horrible', 'bad'], 'longiron4': ['good', 'good', 'good'], 'longiron5': ['horrible', 'horrible', 'bad'], 'longiron6': ['horrible', 'horrible', 'horrible'], 'short1': ['good', 'good', 'good'], 'short2': ['bad', 'horrible', 'horrible'], 'short3': ['good', 'bad', 'good'], 'short4': ['horrible', 'horrible', 'bad'], 'short5': ['good', 'bad', 'good'], 'short6': ['bad', 'good', 'good'], 'short7': ['horrible', 'horrible', 'bad'], 'putt1': ['bad', 'bad', 'good'], 'putt2': ['good', 'good', 'good'], 'putt3': ['good', 'good', 'good'], 'putt4': ['bad', 'good', 'good'], 'putt5': ['bad', 'bad', 'horrible']}\n"
     ]
    }
   ],
   "source": [
    "#displaying crowdsourced values!!!!\n",
    "\n",
    "#open my crowd file for reading\n",
    "p = open(\"crowd.txt\", \"r\")\n",
    "crowd = {}\n",
    "if p.mode == 'r':\n",
    "    #take each line in file and read it \n",
    "    #store these also in a dictionary for more accuracy testing \n",
    "    for line in p:\n",
    "        (key, val1, val2, val3) = line.split()\n",
    "        print(key,val1, val2, val3)\n",
    "        crowd[key] = [val1,val2,val3]\n",
    "print()\n",
    "print(crowd)\n",
    "p.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to show how to display a video \n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "\n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "cap = cv2.VideoCapture('short1-change.avi')\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "# Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "    # Display the resulting frame\n",
    "        cv2.imshow('Frame',frame)\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "  # Break the loop\n",
    "    else: \n",
    "        break\n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    "\n",
    " \n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code to get the values needed to interpret body language\n",
    "def get_bodylang(filename):\n",
    "    #get video file \n",
    "    vs = cv2.VideoCapture(filename)\n",
    "   \n",
    "    # initialize the first frame in the video stream\n",
    "    firstFrame = None\n",
    "    prev = None\n",
    "    #declare values to measure total movement from first frame and frame by frame movements \n",
    "    totalMotion = 0\n",
    "    framebyframe = []\n",
    "    # loop over the frames of the video\n",
    "    while True:\n",
    "        # grab the current frame and initialize the occupied/unoccupied\n",
    "        # text\n",
    "        frame = vs.read()\n",
    "        frame = frame[1]\n",
    "        text = \"Unoccupied\"\n",
    "        # if the frame could not be grabbed, then we have reached the end\n",
    "        # of the video\n",
    "        if frame is None:\n",
    "            break\n",
    "        # resize the frame, convert it to grayscale, and blur it\n",
    "        frame = imutils.resize(frame, width=500)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "        # if the first frame is None, initialize it and make it the prev frame too\n",
    "        if firstFrame is None:\n",
    "            firstFrame = gray\n",
    "            prev = gray\n",
    "            continue\n",
    "        # compute the absolute difference between the current frame and\n",
    "        # first frame\n",
    "        frameDelta = cv2.absdiff(firstFrame, gray)\n",
    "        #append sum of diff pixels between current frame and previous frame\n",
    "        framebyframe.append(cv2.absdiff(prev, gray).sum())\n",
    "        thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "        # dilate the thresholded image to fill in holes, then find contours\n",
    "        # on thresholded image\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # if the contour is too small, ignore it\n",
    "            if cv2.contourArea(c) < 4000:\n",
    "                continue\n",
    "            # keep track of total motion that has happend for each frame\n",
    "            totalMotion += cv2.absdiff(firstFrame, gray).sum()\n",
    "            # compute the bounding box for the contour, draw it on the frame,\n",
    "            # and update the text\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # show the frame and record if the user presses a key\n",
    "       \n",
    "        cv2.imshow(\"With color\", frame)\n",
    "        cv2.imshow(\"Binarized\", thresh)\n",
    "        cv2.imshow(\"Frame Delta\", frameDelta)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # if the `q` key is pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        #make your current frame now the previous frame for the next iteration\n",
    "        prev = frameDelta\n",
    "    # cleanup the camera and close any open windows\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    #print the values you found in both metrics\n",
    "    print('Total difference btw first and last frame:', totalMotion )\n",
    "    print('For each frame by frame difference, here is the max difference:', max(framebyframe),\n",
    "          ', min difference:', min(framebyframe),\n",
    "          ', average distance:', sum(framebyframe)/len(framebyframe), \n",
    "          ', sum of differences:', sum(framebyframe), '\\n')\n",
    "    # return both metrics\n",
    "    return totalMotion, min(framebyframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short1 good\n",
      "Total difference btw first and last frame: 57389022.0\n",
      "For each frame by frame difference, here is the max difference: 32094341 , min difference: 132874 , average distance: 30296385.933333334 , sum of differences: 908891578.0 \n",
      "\n",
      "short2 horrible\n",
      "Total difference btw first and last frame: 2050764219.0\n",
      "For each frame by frame difference, here is the max difference: 37275310 , min difference: 122905 , average distance: 34627608.502702706 , sum of differences: 6406107573.0 \n",
      "\n",
      "short3 bad\n",
      "Total difference btw first and last frame: 813388766.0\n",
      "For each frame by frame difference, here is the max difference: 56875153 , min difference: 634875 , average distance: 52680998.98648649 , sum of differences: 3898393925.0 \n",
      "\n",
      "short4 bad\n",
      "Total difference btw first and last frame: 1149296603.0\n",
      "For each frame by frame difference, here is the max difference: 24151140 , min difference: 9561 , average distance: 21997969.944186047 , sum of differences: 4729563538.0 \n",
      "\n",
      "short5 good\n",
      "Total difference btw first and last frame: 2524797494.0\n",
      "For each frame by frame difference, here is the max difference: 54042619 , min difference: 16692 , average distance: 50512240.76027397 , sum of differences: 7374787151.0 \n",
      "\n",
      "short6 bad\n",
      "Total difference btw first and last frame: 256343769.0\n",
      "For each frame by frame difference, here is the max difference: 36762058 , min difference: 181940 , average distance: 35352841.16 , sum of differences: 3535284116.0 \n",
      "\n",
      "short7 horrible\n",
      "Total difference btw first and last frame: 28595629.0\n",
      "For each frame by frame difference, here is the max difference: 14247872 , min difference: 7960 , average distance: 12720399.837209303 , sum of differences: 1093954386.0 \n",
      "\n",
      "Total distance per shot, sorted: [(28595629.0, 'short7'), (57389022.0, 'short1'), (256343769.0, 'short6'), (813388766.0, 'short3'), (1149296603.0, 'short4'), (2050764219.0, 'short2'), (2524797494.0, 'short5')]\n",
      "\n",
      "tee1 good\n",
      "Total difference btw first and last frame: 3025233218.0\n",
      "For each frame by frame difference, here is the max difference: 31978108 , min difference: 12929 , average distance: 29164902.552238807 , sum of differences: 5862145413.0 \n",
      "\n",
      "tee2 bad\n",
      "Total difference btw first and last frame: 364857511.0\n",
      "For each frame by frame difference, here is the max difference: 21923091 , min difference: 516618 , average distance: 19507478.442307692 , sum of differences: 1014388879.0 \n",
      "\n",
      "tee3 horrible\n",
      "Total difference btw first and last frame: 2706236125.0\n",
      "For each frame by frame difference, here is the max difference: 30903161 , min difference: 402392 , average distance: 27227409.3828125 , sum of differences: 6970216802.0 \n",
      "\n",
      "tee4 horrible\n",
      "Total difference btw first and last frame: 327225405.0\n",
      "For each frame by frame difference, here is the max difference: 28715885 , min difference: 523672 , average distance: 26608771.739583332 , sum of differences: 2554442087.0 \n",
      "\n",
      "tee5 good\n",
      "Total difference btw first and last frame: 1182603.0\n",
      "For each frame by frame difference, here is the max difference: 16260439 , min difference: 11209 , average distance: 15081742.671641791 , sum of differences: 1010476759.0 \n",
      "\n",
      "tee6 bad\n",
      "Total difference btw first and last frame: 2256596213.0\n",
      "For each frame by frame difference, here is the max difference: 39540304 , min difference: 210270 , average distance: 36463194.27840909 , sum of differences: 6417522193.0 \n",
      "\n",
      "tee7 good\n",
      "Total difference btw first and last frame: 1052303561.0\n",
      "For each frame by frame difference, here is the max difference: 35504615 , min difference: 44346 , average distance: 33123999.85135135 , sum of differences: 4902351978.0 \n",
      "\n",
      "Minimum distance per shot, sorted: [(11209, 'tee5'), (12929, 'tee1'), (44346, 'tee7'), (210270, 'tee6'), (402392, 'tee3'), (516618, 'tee2'), (523672, 'tee4')]\n",
      "\n",
      "longiron1 good\n",
      "Total difference btw first and last frame: 1273224202.0\n",
      "For each frame by frame difference, here is the max difference: 27280288 , min difference: 363174 , average distance: 25562905.02094241 , sum of differences: 4882514859.0 \n",
      "\n",
      "longiron2 horrible\n",
      "Total difference btw first and last frame: 784811651.0\n",
      "For each frame by frame difference, here is the max difference: 28069222 , min difference: 171767 , average distance: 26335710.93248945 , sum of differences: 6241563491.0 \n",
      "\n",
      "longiron3 bad\n",
      "Total difference btw first and last frame: 1032547013.0\n",
      "For each frame by frame difference, here is the max difference: 31657466 , min difference: 220037 , average distance: 29797887.97983871 , sum of differences: 7389876219.0 \n",
      "\n",
      "longiron4 good\n",
      "Total difference btw first and last frame: 3392726832.0\n",
      "For each frame by frame difference, here is the max difference: 31436521 , min difference: 209790 , average distance: 28004762.26859504 , sum of differences: 6777152469.0 \n",
      "\n",
      "longiron5 horrible\n",
      "Total difference btw first and last frame: 1481657293.0\n",
      "For each frame by frame difference, here is the max difference: 16577116 , min difference: 5506 , average distance: 11164309.025 , sum of differences: 1786289444.0 \n",
      "\n",
      "longiron6 bad\n",
      "Total difference btw first and last frame: 1547214092.0\n",
      "For each frame by frame difference, here is the max difference: 29312044 , min difference: 711729 , average distance: 25707087.246478874 , sum of differences: 3650406389.0 \n",
      "\n",
      "Minimum distance per shot, sorted: [(5506, 'longiron5'), (171767, 'longiron2'), (209790, 'longiron4'), (220037, 'longiron3'), (363174, 'longiron1'), (711729, 'longiron6')]\n",
      "\n",
      "putt1 horrible\n",
      "Total difference btw first and last frame: 328757994.0\n",
      "For each frame by frame difference, here is the max difference: 21745903 , min difference: 139510 , average distance: 20685932.94623656 , sum of differences: 3847583528.0 \n",
      "\n",
      "putt2 good\n",
      "Total difference btw first and last frame: 282172161.0\n",
      "For each frame by frame difference, here is the max difference: 29649571 , min difference: 42899 , average distance: 28300658.40588235 , sum of differences: 4811111929.0 \n",
      "\n",
      "putt3 good\n",
      "Total difference btw first and last frame: 672837137.0\n",
      "For each frame by frame difference, here is the max difference: 27834490 , min difference: 133496 , average distance: 26708750.11790393 , sum of differences: 6116303777.0 \n",
      "\n",
      "putt4 good\n",
      "Total difference btw first and last frame: 407696533.0\n",
      "For each frame by frame difference, here is the max difference: 32200776 , min difference: 16882 , average distance: 30941076.34751773 , sum of differences: 4362691765.0 \n",
      "\n",
      "putt5 bad\n",
      "Total difference btw first and last frame: 6130234322.0\n",
      "For each frame by frame difference, here is the max difference: 37777196 , min difference: 78033 , average distance: 33380688.870535713 , sum of differences: 14954548614.0 \n",
      "\n",
      "Minimum distance per shot, sorted: [(16882, 'putt4'), (42899, 'putt2'), (78033, 'putt5'), (133496, 'putt3'), (139510, 'putt1')]\n"
     ]
    }
   ],
   "source": [
    "#code to loop through all videos and get their predictions\n",
    "\n",
    "#the amount of videos in each category\n",
    "sizeShort = 7\n",
    "sizeTee = 7\n",
    "sizeLong = 6\n",
    "sizePutt = 5\n",
    "#dictionaries to hold predictions and sort values for comparison\n",
    "short_summary = {}\n",
    "long_summary = {}\n",
    "putt_summary = {}\n",
    "tee_summary = {}\n",
    "predicts = {}\n",
    "\n",
    "#loop through each of the videos in a certain category \n",
    "for i in range(1,sizeShort+1):\n",
    "    #create filename from number and description\n",
    "    filename = 'short'+ str(i)+'.mov'\n",
    "    #print what I'm aiming for\n",
    "    print('short'+str(i), truths['short'+str(i)])\n",
    "    #get the two metrics returned below\n",
    "    summ, minf = get_bodylang(filename)\n",
    "    # assign the appropriate one per shot\n",
    "    short_summary['short'+str(i)] = summ\n",
    "#sort the values based on their metric\n",
    "sorted_summ = sorted((value, key) for (key,value) in short_summary.items())\n",
    "print('Total distance per shot, sorted:' , sorted_summ)\n",
    "#loop through the sorted values and assign them based on relative position\n",
    "for item in range(0, len(sorted_summ)):\n",
    "    # 2 smallest get 'good'\n",
    "    if item < 2:\n",
    "        predicts[sorted_summ[item][1]] = 'good'\n",
    "    # 2 largest get 'horrible'\n",
    "    elif item > 4:\n",
    "        predicts[sorted_summ[item][1]] = 'horrible'\n",
    "    # whatever is in the middle gets 'bad'\n",
    "    else:\n",
    "        predicts[sorted_summ[item][1]] = 'bad'\n",
    "print()\n",
    "\n",
    "#same logic for the rest, \n",
    "#just with the different categories and different cutoffs \n",
    "#based on how many videos are in each category\n",
    "\n",
    "#tee shots\n",
    "for i in range(1,sizeTee+1):\n",
    "    filename = 'tee'+ str(i)+'.mov'\n",
    "    print('tee'+str(i), truths['tee'+str(i)])\n",
    "    summ, minf = get_bodylang(filename)\n",
    "    tee_summary['tee'+str(i)] = minf\n",
    "sorted_summ = sorted((value, key) for (key,value) in tee_summary.items())\n",
    "print('Minimum distance per shot, sorted:' , sorted_summ)\n",
    "for item in range(0, len(sorted_summ)):\n",
    "    if item < 3:\n",
    "        predicts[sorted_summ[item][1]] = 'good' \n",
    "    elif item > 4:\n",
    "        predicts[sorted_summ[item][1]] = 'horrible'\n",
    "    else:\n",
    "        predicts[sorted_summ[item][1]] = 'bad'\n",
    "print()\n",
    "\n",
    "#long irons \n",
    "for i in range(1,sizeLong+1):\n",
    "    filename = 'longiron'+ str(i)+'.mov'\n",
    "    print('longiron'+str(i), truths['longiron'+str(i)])\n",
    "    summ, minf = get_bodylang(filename)\n",
    "    long_summary['longiron'+str(i)] = minf\n",
    "sorted_summ = sorted((value, key) for (key,value) in long_summary.items())\n",
    "print('Minimum distance per shot, sorted:' , sorted_summ)\n",
    "for item in range(0, len(sorted_summ)):\n",
    "    if item < 2:\n",
    "        predicts[sorted_summ[item][1]] = 'horrible' \n",
    "    elif item > 3:\n",
    "        predicts[sorted_summ[item][1]] = 'good'\n",
    "    else:\n",
    "        predicts[sorted_summ[item][1]] = 'bad'\n",
    "print()\n",
    "\n",
    "#putts\n",
    "for i in range(1,sizePutt+1):\n",
    "    filename = 'putt'+ str(i)+'.mov'\n",
    "    print('putt'+str(i), truths['putt'+str(i)])\n",
    "    summ, minf = get_bodylang(filename)\n",
    "    putt_summary['putt'+str(i)] = minf\n",
    "sorted_summ = sorted((value, key) for (key,value) in putt_summary.items())\n",
    "print('Minimum distance per shot, sorted:' , sorted_summ)\n",
    "for item in range(0, len(sorted_summ)):\n",
    "    if item < 1:\n",
    "        predicts[sorted_summ[item][1]] = 'bad' \n",
    "    elif item > 3:\n",
    "        predicts[sorted_summ[item][1]] = 'horrible'\n",
    "    else:\n",
    "        predicts[sorted_summ[item][1]] = 'good'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Note: </h3>\n",
    "When I first saw the values for each category of shots, I was not sure if I should have made concrete cutoffs for each classification or more relative cutoffs for each classification. But I ended up using relative cutoffs because shots are measured relative to each other for a player all the time. If you are a golfer that always hits the fairways, like Tiger, hitting a shot in the rough is a bad shot. And in general golf shots are relative, if you hit a shot further than your competitor, but it is in the water hazard, hitting that shot longer was not a good thing. That is why golf is about placement and accuracy rather than yardage. Also, this applies to emotional responses. When you have a shot that is bad, but it is earlier in the round, you are more likely to let that shot roll off your back. Later in the round, that bad shot may become more frustrating, so the body language may be more exaggerated. Also, once I made the charts below, I saw how different the values were for each category, so that meant that I would have to come up with 4 sets of cutoffs, which seemed too labor intensive for me. \n",
    "<br><br>\n",
    "\n",
    "Also, I got help making the box plots from this link: http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/ and I think that I have permission to use this because it is a tutorial on how to create boxplots from a widely used python package, it didn't aid directly in any analysis at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAFlCAYAAAAamLmIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAc6ElEQVR4nO3df7Rd5V3n8fenSSm0ikAbKCb8qMtMpUZL6R0abZ1pRSFUlqkzomCVtBMn2kGmHWdGqIwTS2UNzB/W0rEoI0hwVSj9JRmGNkaaqlm2lIsiUCqSQYFbfqUNpVRsS+l3/jhP2kN6cu9NCDn3uff9Wuuss/d3P3s/zwmHsz732Xufk6pCkiSpN88Z9wAkSZL2hiFGkiR1yRAjSZK6ZIiRJEldMsRIkqQuGWIkSVKXFo97APvai170ojr22GPHPQxJkrQP3HLLLZ+vqiWjts27EHPssccyOTk57mFIkqR9IMm9u9vm6SRJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjKSuXH311axYsYJFixaxYsUKrr766nEPSdKYzLufHZA0f1199dWcf/75XH755bzmNa9h69atrF27FoAzzzxzzKOTtL+lqsY9hn1qYmKi/O0kaX5asWIF73nPe3jd6173zdqWLVs455xzuOOOO8Y4MknPliS3VNXEyG2GGEm9WLRoEV/5yld47nOf+83ak08+yYEHHshTTz01xpFJerZMF2K8JkZSN4477ji2bt36tNrWrVs57rjjxjQiSeM0Y4hJ8tIktw49vpTkbUkOS7I5yd3t+dDWPkkuSbItyW1JThg61prW/u4ka4bqr0xye9vnkiRp9ZF9SFqYzj//fNauXcuWLVt48skn2bJlC2vXruX8888f99AkjcGMIaaq7qqq46vqeOCVwBPAR4DzgBurajlwY1sHOBVY3h7rgEthEEiA9cCrgBOB9UOh5NLWdud+q1p9d31IWoDOPPNMLrzwQs455xwOPPBAzjnnHC688EIv6pUWqD26JibJycD6qnp1kruA11bVg0mOBD5RVS9N8vtt+eq2z13Aa3c+quqXWv33gU+0x5aq+r5WP3Nnu931Md0YvSZGkqT5Y19eE3MGsPNLGY6oqgcB2vPhrb4UuH9on6lWm64+NaI+XR9Pk2Rdkskkk9u3b9/DlyRJkno06xCT5ADgJ4EPzNR0RK32oj5rVXVZVU1U1cSSJUv2ZFdJktSpPZmJORX466p6uK0/3E7x0J4fafUp4Kih/ZYBD8xQXzaiPl0fkiRpgduTEHMm3zqVBLAR2HmH0RrguqH6We0upZXAY+1U0Cbg5CSHtgt6TwY2tW2PJ1nZ7ko6a5djjepDkiQtcLP62YEkzwd+HPilofJFwLVJ1gL3Aae3+g3A64FtDO5kejNAVe1I8k7g5tbugqra0ZbfAlwJHAR8tD2m60OSJC1wfmOvJEmas/zGXkmSNO8YYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLhliJElSlwwxkiSpS4YYSZLUJUOMJEnqkiFGkiR1yRAjSZK6ZIiRJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLs0qxCQ5JMkHk/xdks8m+aEkhyXZnOTu9nxoa5sklyTZluS2JCcMHWdNa393kjVD9Vcmub3tc0mStPrIPiRJkmY7E/Nu4GNV9X3Ay4HPAucBN1bVcuDGtg5wKrC8PdYBl8IgkADrgVcBJwLrh0LJpa3tzv1Wtfru+pAkSQvcjCEmycHAvwIuB6iqr1XVF4HVwIbWbAPwhra8GriqBj4FHJLkSOAUYHNV7aiqR4HNwKq27eCq+mRVFXDVLsca1YckSVrgZjMT8z3AduAPk/xNkj9I8gLgiKp6EKA9H97aLwXuH9p/qtWmq0+NqDNNH0+TZF2SySST27dvn8VLkiRJvZtNiFkMnABcWlWvAP6J6U/rZESt9qI+a1V1WVVNVNXEkiVL9mRXSZLUqdmEmClgqqpuausfZBBqHm6ngmjPjwy1P2po/2XAAzPUl42oM00fkiRpgZsxxFTVQ8D9SV7aSicBdwIbgZ13GK0BrmvLG4Gz2l1KK4HH2qmgTcDJSQ5tF/SeDGxq2x5PsrLdlXTWLsca1YckSVrgFs+y3TnA+5IcANwDvJlBALo2yVrgPuD01vYG4PXANuCJ1paq2pHkncDNrd0FVbWjLb8FuBI4CPhoewBctJs+JEnSApfBDUHzx8TERE1OTo57GJIkaR9IcktVTYza5jf2SpKkLhliJElSlwwxkiSpS4YYSZLUJUOMJEnqkiFGkiR1yRAjSZK6ZIiRJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLhliJElSlxaPewCSJM1lSfZLP1W1X/qZTwwxkiRNY0/DRRIDyX7i6SRJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV2aVYhJ8o9Jbk9ya5LJVjssyeYkd7fnQ1s9SS5Jsi3JbUlOGDrOmtb+7iRrhuqvbMff1vbNdH1IkiTtyUzM66rq+KqaaOvnATdW1XLgxrYOcCqwvD3WAZfCIJAA64FXAScC64dCyaWt7c79Vs3QhyRJWuCeyemk1cCGtrwBeMNQ/aoa+BRwSJIjgVOAzVW1o6oeBTYDq9q2g6vqkzW4sf6qXY41qg9JkrTAzTbEFPCnSW5Jsq7VjqiqBwHa8+GtvhS4f2jfqVabrj41oj5dH0+TZF2SySST27dvn+VLkiRJPZvtN/a+uqoeSHI4sDnJ303TdtT3M9de1Getqi4DLgOYmJjwaxIlSVoAZjUTU1UPtOdHgI8wuKbl4XYqiPb8SGs+BRw1tPsy4IEZ6stG1JmmD0mStMDNGGKSvCDJd+5cBk4G7gA2AjvvMFoDXNeWNwJntbuUVgKPtVNBm4CTkxzaLug9GdjUtj2eZGW7K+msXY41qg9JkrTAzeZ00hHAR9pdz4uBP66qjyW5Gbg2yVrgPuD01v4G4PXANuAJ4M0AVbUjyTuBm1u7C6pqR1t+C3AlcBDw0fYAuGg3fUiSpAUu8+2XNicmJmpycnLcw5AkLVD+ivW+leSWoa93eRq/sVeSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJIWjCOXHU2SZ/UBPOt9HLns6DH/S84Ns/kVa0mS5oWHPnc/x5x7/biH8Yzde/Fp4x7CnOBMjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLhliJElSlwwxkiSpS4YYSZLUJUOMJEnqkiFGkiR1yRAjSZK6ZIiRJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV2adYhJsijJ3yS5vq2/JMlNSe5O8v4kB7T689r6trb92KFjvL3V70pyylB9VattS3LeUH1kH5IkSXsyE/NW4LND6xcD76qq5cCjwNpWXws8WlXfC7yrtSPJy4AzgO8HVgHvbcFoEfC7wKnAy4AzW9vp+pAkSQvcrEJMkmXATwB/0NYD/CjwwdZkA/CGtry6rdO2n9TarwauqaqvVtU/ANuAE9tjW1XdU1VfA64BVs/QhyRJWuBmOxPzO8CvAd9o6y8EvlhVX2/rU8DStrwUuB+gbX+stf9mfZd9dlefro+nSbIuyWSSye3bt8/yJUmSpJ7NGGKSnAY8UlW3DJdHNK0Ztu2r+rcXqy6rqomqmliyZMmoJpIkaZ5ZPIs2rwZ+MsnrgQOBgxnMzBySZHGbKVkGPNDaTwFHAVNJFgPfBewYqu80vM+o+uen6UOSJC1wM87EVNXbq2pZVR3L4MLcj1fVG4EtwE+3ZmuA69ryxrZO2/7xqqpWP6PdvfQSYDnwaeBmYHm7E+mA1sfGts/u+pAkSQvcM/memHOBX02yjcH1K5e3+uXAC1v9V4HzAKrqM8C1wJ3Ax4Czq+qpNsvyK8AmBnc/XdvaTteHJEla4GZzOumbquoTwCfa8j0M7izatc1XgNN3s/+FwIUj6jcAN4yoj+xDkiTJb+yVJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC7t0c8OSNKzIcl+6Wfwu7KS5gtDjKSx25twkcRQIi1wnk6SJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjKR97shlR5PkWX0Az3ofRy47esz/kpKms3jcA5A0/zz0ufs55tzrxz2MZ+zei08b9xAkTcOZGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLs0YYpIcmOTTSf42yWeSvKPVX5LkpiR3J3l/kgNa/XltfVvbfuzQsd7e6nclOWWovqrVtiU5b6g+sg9JkqTZzMR8FfjRqno5cDywKslK4GLgXVW1HHgUWNvarwUerarvBd7V2pHkZcAZwPcDq4D3JlmUZBHwu8CpwMuAM1tbpulDkiQtcDOGmBr4clt9bnsU8KPAB1t9A/CGtry6rdO2n5TB12uuBq6pqq9W1T8A24AT22NbVd1TVV8DrgFWt31214ckSVrgZnVNTJsxuRV4BNgM/D/gi1X19dZkCljalpcC9wO07Y8BLxyu77LP7uovnKaPXce3Lslkksnt27fP5iVJkqTOzSrEVNVTVXU8sIzBzMlxo5q15+xm276qjxrfZVU1UVUTS5YsGdVEkiTNM3t0d1JVfRH4BLASOCTJzt9eWgY80JangKMA2vbvAnYM13fZZ3f1z0/ThyRJWuBmc3fSkiSHtOWDgB8DPgtsAX66NVsDXNeWN7Z12vaPV1W1+hnt7qWXAMuBTwM3A8vbnUgHMLj4d2PbZ3d9SJKkBW42v2J9JLCh3UX0HODaqro+yZ3ANUl+C/gb4PLW/nLgj5JsYzADcwZAVX0mybXAncDXgbOr6imAJL8CbAIWAVdU1Wfasc7dTR+SJGmBmzHEVNVtwCtG1O9hcH3MrvWvAKfv5lgXAheOqN8A3DDbPiRJkvzGXkmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLhliJElSlwwxkiSpS4YYSZLUJUOMJEnqkiFGkiR1yRAjSZK6ZIiRJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXTLESJKkLi0e9wAkzT+1/mDg58Y9jGdu/cHjHoGkaRhiJO1zeceXOObc68c9jGfs3otPo35z3KOQtDuGGEnSguEs4fwyY4hJchRwFfBi4BvAZVX17iSHAe8HjgX+EfiZqno0SYB3A68HngDeVFV/3Y61Bvhv7dC/VVUbWv2VwJXAQcANwFurqnbXxzN+1ZKkBclZwvllNhf2fh34z1V1HLASODvJy4DzgBurajlwY1sHOBVY3h7rgEsBWiBZD7wKOBFYn+TQts+lre3O/Va1+u76kCRJC9yMIaaqHtw5k1JVjwOfBZYCq4ENrdkG4A1teTVwVQ18CjgkyZHAKcDmqtrRZlM2A6vatoOr6pNVVQxmfYaPNaoPSZK0wO3RLdZJjgVeAdwEHFFVD8Ig6ACHt2ZLgfuHdptqtenqUyPqTNPHruNal2QyyeT27dv35CVJkqROzTrEJPkO4EPA26rqS9M1HVGrvajPWlVdVlUTVTWxZMmSPdlVkiR1alYhJslzGQSY91XVh1v54XYqiPb8SKtPAUcN7b4MeGCG+rIR9en6kCRJC9yMIabdbXQ58Nmq+u2hTRuBNW15DXDdUP2sDKwEHmungjYBJyc5tF3QezKwqW17PMnK1tdZuxxrVB+SJGmBm833xLwa+AXg9iS3ttqvAxcB1yZZC9wHnN623cDg9uptDG6xfjNAVe1I8k7g5tbugqra0Zbfwrdusf5oezBNH5IkaYGbMcRU1VZGX7cCcNKI9gWcvZtjXQFcMaI+CawYUf/CqD4kSZL8AUhJktQlQ4wkSeqSv50kaZ978dKjuPfi08Y9jGfsxUuPmrmRpLExxEja5x6cuu9Z7yMJg0vwJC1Unk6SJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSurR43APQ/pNkv/RTVfulH0nSwmaIWUD2JlwkMZRIkuYkTyd16shlR5PkWX8Az3ofRy47esz/mpKkHjkT06mHPnc/x5x7/biHsU/ce/Fp4x6CJKlDzsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC7NGGKSXJHkkSR3DNUOS7I5yd3t+dBWT5JLkmxLcluSE4b2WdPa351kzVD9lUlub/tckvZd97vrQ5IkCWY3E3MlsGqX2nnAjVW1HLixrQOcCixvj3XApTAIJMB64FXAicD6oVByaWu7c79VM/QhSZI0c4ipqr8AduxSXg1saMsbgDcM1a+qgU8BhyQ5EjgF2FxVO6rqUWAzsKptO7iqPlmDn0q+apdjjepDkiRpr6+JOaKqHgRoz4e3+lLg/qF2U602XX1qRH26Pr5NknVJJpNMbt++fS9fkiRJ6sm+vrA3I2q1F/U9UlWXVdVEVU0sWbJkT3eXJEkd2tsQ83A7FUR7fqTVp4CjhtotAx6Yob5sRH26PiRJkvY6xGwEdt5htAa4bqh+VrtLaSXwWDsVtAk4Ocmh7YLek4FNbdvjSVa2u5LO2uVYo/qQJEli8UwNklwNvBZ4UZIpBncZXQRcm2QtcB9wemt+A/B6YBvwBPBmgKrakeSdwM2t3QVVtfNi4bcwuAPqIOCj7cE0fUiSJM0cYqrqzN1sOmlE2wLO3s1xrgCuGFGfBFaMqH9hVB+SJEngN/ZKkqROGWIkSVKXDDGSJKlLhhhJktSlGS/s1dxU6w8Gfm7cw9g31h887hFIkjpkiOlU3vEljjn3+nEPY5+49+LTqN8c9ygkSb3xdJIkSeqSIUaSJHXJECNJkrpkiJEkSV0yxEiSpC4ZYiRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIhRpIkdckQI0mSumSIkSRJXfIHIDv14qVHce/Fp417GPvEi5ceNe4hSJI6ZIjp1INT9+2XfpJQVfulL0mS9oSnkyRJUpcMMZIkqUuGGEmS1CVDjCRJ6pIX9kqSFoz5cmend3UOGGIkSQvG/riz07s69x9PJ0mSpC4ZYiRJUpcMMZIkqUteE7OAJNkv+3kuWJK0PxhiFhDDhSRpPvF0kiRJ6pIzMZIkTWNvTsXvzT7Olu+5OT8Tk2RVkruSbEty3rjHI0laWKpqvzy05+b0TEySRcDvAj8OTAE3J9lYVXeOd2SS9iUvOpe0N+Z0iAFOBLZV1T0ASa4BVgOGGGkeMVxI2htz/XTSUuD+ofWpVnuaJOuSTCaZ3L59+34bnCRJGp+5HmJGzRV/259sVXVZVU1U1cSSJUv2w7AkSdK4zfUQMwUM/1TnMuCBMY1FkiTNIXM9xNwMLE/ykiQHAGcAG8c8JkmSNAfM6Qt7q+rrSX4F2AQsAq6oqs+MeViSJGkOmNMhBqCqbgBuGPc4JEnS3DLXTydJkiSNZIiRJEldMsRIkqQuGWIkSVKXDDGSJKlLhhhJktQlQ4wkSepS5tuvxybZDtw77nHMIy8CPj/uQUgj+N7UXOV7c986pqpG/jDivAsx2reSTFbVxLjHIe3K96bmKt+b+4+nkyRJUpcMMZIkqUuGGM3ksnEPQNoN35uaq3xv7ideEyNJkrrkTIwkSeqSIWaBSfJUkluT3JHkA0meP0P7NyX57qH1t820j5Tky/uxryuT/PT+6k/9S/LC9jl4a5KHknxuaP2AZ3DcVye5qR3ns0l+o9V/K8nb9uA4hyX55b0dx0JiiFl4/rmqjq+qFcDXgJn+R3kT8N1D628DDDGa85IsGvcYNDdV1Rfa5+DxwO8B79q5XlVfewaH3gCsbcddAXxoL49zGDN/NgtDzEL3l8D3Jjk2yR07i0n+S5LfbH/dTgDva39ZvJVBoNmSZEuSRe2v4DuS3J7kP43pdagDSY5JcmOS29rz0a1+ZZJLkvxVknt2zqokeU6S9yb5TJLrk9ww04xLkn9M8t+TbAVOT3J8kk+1Pj+S5NDW7hNJLk7y6SR/n+RHWv37W+3Wts/yZ/mfRXNMkjVD74H3JnlOq5+a5JNJ/jrJ+5O8YMTuS4CHAKrqqaq6c2jbDyT58/YeP3uov19rn6F3JDmnlS8CXtrGcFGSpUm2Ds2i//Cz9PK7Y4hZoJIsBk4Fbt9dm6r6IDAJvLH9hfJu4AHgdVX1OuB4YGlVraiqHwD+cD8MXf36X8BVVfWDwPuAS4a2HQm8BjiNwQc4wL8BjgV+APhF4Idm2c9Xquo1VXUNcBVwbuvzdmD9ULvFVXUig9nFnfVfBt7d/pKeAKb26BWqa0lWAD8F/HB7DywGzkhyOHAecFJVnQDcBrx1xCF+B7g7yYeT/Pskzxva9i+AHwdWAhe0PwJPBN4InMjg/f0fkvxg6+uu9rl7HvDzwP9pY3p5618M/gNpYTkoya1t+S+By3n66aI9cQ/wPUneA/xf4E/3wfg0f/0Qg2AC8EfA/xza9idV9Q3gziRHtNprgA+0+kNJtsyyn/cDJPku4JCq+vNW3wB8YKjdh9vzLQzCEsAngfOTLAM+XFV3z7JPzQ8/BvxLYDIJwEHA/cATwMuAv2r1A4Ctu+5cVeuT/BFwMnAW8LPtmADXt1NVjyTZwWDW5keAD1XVEwBJ/oTB+37Xz9Kbgd9PciCD/1f+dp+94s4ZYhaef25p/puSfJ2nz8odOJsDVdWjSV4OnAKcDfwM8O/21UA17w1/v8NXh5azy/Oe+qdZttvZ51O0z8Kq+uMkNwE/AWxK8otV9fG9HIf6E+CKqvqNpxWTnwI+VlW/MNMBqmobsC3J/wa+0MI0PP09vvM9N6v3eFV9PMlrGbwv35fkf1TV+2az73zn6SQBPAwc3q7Yfx6DKf2dHge+c9R6khcBz6mqDwG/AZywn8arPv0VcEZbfiMj/pLdxVbg37ZrY44AXrsnnVXVY8CjO693AX4B+PNpdiHJ9wD3VNUlwEbgB/ekT3Xvz4CfaZ9tO+9iOprBe/dft/cHSV4w6nqpJD+RNlXD4PTRVxl8Zu7OXwA/leSgJN8BrGYwQ/60z90kxwAPVdVlwJXAK57Zy5w/nIkRVfVkkguAm4B/AP5uaPOVwO8l+WcGpwMuAz6a5EEG1xL84c4L34C3779Ra457fpLh60l+G/iPwBVJ/iuwHXjzDMf4EHAScAfw9wzen4/t4TjWMHj/Pp/B6c+Z+vxZ4OeTPMngAs0L9rA/dayqbk/yDuDP2ufak8AvV9XNSdYC78+3bsH+dWDX041vAt6V5Im2789V1Te+lWu+rb9PJ7mawekigEur6nYY/IhkktsZnKr/e+BX2/vyywyukRF+Y6+kOSzJd1TVl5O8EPg08Oqqemjc45I0NzgTI2kuuz7JIQwupHynAUbSMGdiJElSl7ywV5IkdckQI0mSumSIkSRJXTLESJKkLhliJElSlwwxkiSpS/8fYYG+YXPIU3YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#i want to visualize my data\n",
    "\n",
    "#make each set of summaries integers instead of strings \n",
    "short = [ int(x) for x in short_summary.values() ]\n",
    "tee = [ int(x) for x in tee_summary.values() ]\n",
    "putt = [ int(x) for x in putt_summary.values() ]\n",
    "long = [ int(x) for x in long_summary.values() ]\n",
    "summaries = [putt,long,tee]\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(summaries, patch_artist=True)\n",
    "ax.set_xticklabels(['Putts', 'Long Irons', 'Tee Shots'])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(short, patch_artist=True)\n",
    "ax.set_xticklabels(['Short Game'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation: </h2>\n",
    "<br>\n",
    "I evaluated my system by comparing its results/predictions against the ‘true’ metric of a shot that I assigned to each video. I will give each clip an indication if the resulting shot was ‘good’, ‘bad’, or ‘horrible’. And within the groups of videos, I hoped to have my system classify at least 80% of the videos with the same label that I gave them, but it ended up being more like 70% with each category having at least 60%, which I wouldn't say is too bad. I also tuned to this measure so at my first run it was around 12/25 (48%) overall and I improved it by playing with the metrics and messing which the classifications relative to each other based on the behavior I observed (explained in the 'Code' portion). These behaviorss mapped really well onto the actual data in a way that was surprising given that my data was limited and fairly unconstriained, which could have picked up a bunch of noise and made it difficult to draw any conclusions. Even in some cases where I wasn't sure that the conputer would be able to classify it correctly because the video was short and didn't seem to have much to draw a conclusion from, like tee5.mov, or the video had movements that could have easily been classified as a different label, like tee6.mov, were still classified correctly. I think in most cases, the two videos deemed incorrectly labeled by my algorithm just had to be switched and it would have been correct. \n",
    "<br><br>\n",
    "I also surveyed my parents and family members who also play golf and see if they can give a metric of ‘good’, ‘bad’, or ‘horrible’ to each shot as well, but just based on the body language clips alone, so that I can see if my system could be as good as humans. I explained to them the movements I attributed to each class and let them watch the clips. I took 3 'guesses' from my family to see what they though of the body language. It was interesting, because based off of the body language alone, many of my family members guesses were far off from my truth assessments. While some clips were obvious that he was happy or frustrated, some of them were ambiguous, and it confused my family members. Overall, the accuracy between my system and the crowdsourcing was around 43%, which is not that great. But when I was thinking about how bad my family's predictions were, I wanted to see how they related to the truth values as well. Their accuracy was around 57% which wasn't too far from the system's accuracy. I guess one could say that this system was almost as good as a crowd that had to make decisions from limited scope as well. \n",
    "<br><br>\n",
    "This just goes to show that because he keeps a tight leash on his emotions on the course, Tiger is very hard to read to humans and computers. I also suspect that his general attitude for the day or the way he is currently playing the round can affect the precision in what his body language means, since I found that in clips from later parts of the tournament he was more expressive, which may have indicated frustration. I also tuned my system to my truth values, and since my family members were often diverged from my assigned values, that could explain some of the discrepancy as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: {'short7': 'good', 'short1': 'good', 'short6': 'bad', 'short3': 'bad', 'short4': 'bad', 'short2': 'horrible', 'short5': 'horrible', 'tee5': 'good', 'tee1': 'good', 'tee7': 'good', 'tee6': 'bad', 'tee3': 'bad', 'tee2': 'horrible', 'tee4': 'horrible', 'longiron5': 'horrible', 'longiron2': 'horrible', 'longiron4': 'bad', 'longiron3': 'bad', 'longiron1': 'good', 'longiron6': 'good', 'putt4': 'bad', 'putt2': 'good', 'putt5': 'good', 'putt3': 'good', 'putt1': 'horrible'}\n",
      "\n",
      "Assigned truth values: {'tee1': 'good', 'tee2': 'bad', 'tee3': 'horrible', 'tee4': 'horrible', 'tee5': 'good', 'tee6': 'bad', 'tee7': 'good', 'longiron1': 'good', 'longiron2': 'horrible', 'longiron3': 'bad', 'longiron4': 'good', 'longiron5': 'horrible', 'longiron6': 'bad', 'short1': 'good', 'short2': 'horrible', 'short3': 'bad', 'short4': 'bad', 'short5': 'good', 'short6': 'bad', 'short7': 'horrible', 'putt1': 'horrible', 'putt2': 'good', 'putt3': 'good', 'putt4': 'good', 'putt5': 'bad'}\n",
      "incorrect value: short7\n",
      "incorrect value: short5\n",
      "incorrect value: tee3\n",
      "incorrect value: tee2\n",
      "incorrect value: longiron4\n",
      "incorrect value: longiron6\n",
      "incorrect value: putt4\n",
      "incorrect value: putt5\n",
      "\n",
      "Your system got 17 predictions right out of 25\n"
     ]
    }
   ],
   "source": [
    "# compare with truth values\n",
    "print('Predicted values:', predicts)\n",
    "print()\n",
    "print('Assigned truth values:', truths)\n",
    "\n",
    "correct = 0\n",
    "for result in predicts:\n",
    "    if predicts[result] == truths[result]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        print('incorrect value:',result)\n",
    "print()\n",
    "print('Your system got', str(correct), 'predictions right out of', str(len(predicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: {'short7': 'good', 'short1': 'good', 'short6': 'bad', 'short3': 'bad', 'short4': 'bad', 'short2': 'horrible', 'short5': 'horrible', 'tee5': 'good', 'tee1': 'good', 'tee7': 'good', 'tee6': 'bad', 'tee3': 'bad', 'tee2': 'horrible', 'tee4': 'horrible', 'longiron5': 'horrible', 'longiron2': 'horrible', 'longiron4': 'bad', 'longiron3': 'bad', 'longiron1': 'good', 'longiron6': 'good', 'putt4': 'bad', 'putt2': 'good', 'putt5': 'good', 'putt3': 'good', 'putt1': 'horrible'}\n",
      "\n",
      "Crowdsourced values: {'tee1': ['good', 'good', 'bad'], 'tee2': ['good', 'good', 'good'], 'tee3': ['bad', 'horrible', 'horrible'], 'tee4': ['horrible', 'horrible', 'horrible'], 'tee5': ['good', 'good', 'good'], 'tee6': ['horrible', 'horrible', 'bad'], 'tee7': ['good', 'good', 'good'], 'longiron1': ['bad', 'good', 'bad'], 'longiron2': ['bad', 'bad', 'good'], 'longiron3': ['horrible', 'horrible', 'bad'], 'longiron4': ['good', 'good', 'good'], 'longiron5': ['horrible', 'horrible', 'bad'], 'longiron6': ['horrible', 'horrible', 'horrible'], 'short1': ['good', 'good', 'good'], 'short2': ['bad', 'horrible', 'horrible'], 'short3': ['good', 'bad', 'good'], 'short4': ['horrible', 'horrible', 'bad'], 'short5': ['good', 'bad', 'good'], 'short6': ['bad', 'good', 'good'], 'short7': ['horrible', 'horrible', 'bad'], 'putt1': ['bad', 'bad', 'good'], 'putt2': ['good', 'good', 'good'], 'putt3': ['good', 'good', 'good'], 'putt4': ['bad', 'good', 'good'], 'putt5': ['bad', 'bad', 'horrible']}\n",
      "\n",
      "Your system got 32 predictions right out of 75\n"
     ]
    }
   ],
   "source": [
    "#compare with crowdsourced values \n",
    "print('Predicted values:',predicts)\n",
    "print()\n",
    "print('Crowdsourced values:', crowd)\n",
    "\n",
    "correct = 0\n",
    "for result in crowd:\n",
    "    for vals in crowd[result]:\n",
    "        if predicts[result] == vals:\n",
    "            correct+=1\n",
    "\n",
    "print()\n",
    "print('Your system got', str(correct), 'predictions right out of', str(len(predicts)*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned truth values: {'tee1': 'good', 'tee2': 'bad', 'tee3': 'horrible', 'tee4': 'horrible', 'tee5': 'good', 'tee6': 'bad', 'tee7': 'good', 'longiron1': 'good', 'longiron2': 'horrible', 'longiron3': 'bad', 'longiron4': 'good', 'longiron5': 'horrible', 'longiron6': 'bad', 'short1': 'good', 'short2': 'horrible', 'short3': 'bad', 'short4': 'bad', 'short5': 'good', 'short6': 'bad', 'short7': 'horrible', 'putt1': 'horrible', 'putt2': 'good', 'putt3': 'good', 'putt4': 'good', 'putt5': 'bad'}\n",
      "\n",
      "Crowdsourced values: {'tee1': ['good', 'good', 'bad'], 'tee2': ['good', 'good', 'good'], 'tee3': ['bad', 'horrible', 'horrible'], 'tee4': ['horrible', 'horrible', 'horrible'], 'tee5': ['good', 'good', 'good'], 'tee6': ['horrible', 'horrible', 'bad'], 'tee7': ['good', 'good', 'good'], 'longiron1': ['bad', 'good', 'bad'], 'longiron2': ['bad', 'bad', 'good'], 'longiron3': ['horrible', 'horrible', 'bad'], 'longiron4': ['good', 'good', 'good'], 'longiron5': ['horrible', 'horrible', 'bad'], 'longiron6': ['horrible', 'horrible', 'horrible'], 'short1': ['good', 'good', 'good'], 'short2': ['bad', 'horrible', 'horrible'], 'short3': ['good', 'bad', 'good'], 'short4': ['horrible', 'horrible', 'bad'], 'short5': ['good', 'bad', 'good'], 'short6': ['bad', 'good', 'good'], 'short7': ['horrible', 'horrible', 'bad'], 'putt1': ['bad', 'bad', 'good'], 'putt2': ['good', 'good', 'good'], 'putt3': ['good', 'good', 'good'], 'putt4': ['bad', 'good', 'good'], 'putt5': ['bad', 'bad', 'horrible']}\n",
      "\n",
      "Your family got 43 assigned values correct right out of 75 attempts\n"
     ]
    }
   ],
   "source": [
    "#compare truth values with crowdsourced values\n",
    "#how good are people at predicting body language?\n",
    "\n",
    "print('Assigned truth values:',truths)\n",
    "print()\n",
    "print('Crowdsourced values:', crowd)\n",
    "\n",
    "correct = 0\n",
    "for result in crowd:\n",
    "    for vals in crowd[result]:\n",
    "        if truths[result] == vals:\n",
    "            correct+=1\n",
    "\n",
    "print()\n",
    "print('Your family got', str(correct), 'assigned values correct right out of', str(len(truths)*3), 'attempts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reflection: </h2>\n",
    "<br>\n",
    "This project showed me how difficult it is to try to detect emotional body language, even for humans. In my proposal, I was thinking that my system would be able to achieve 80% accuracy, but even the humans couldn't get that accuracy from small clips.  Tiger was also probably the wrong subject for this project also, since he has been known for being very stoic during his playing. He rarely talks to his competitors and stays laser-focused for the whole round. If I had done another player like Phil Mickleson, who is more open on the course and is extremely expressive, maybe some things would have changed. Also, body movement is not the only way to measure body language, but I think it would be interesting to also look at facial expressions. That would be very difficult in a golf setting, but I am willing to bet that facial expressions may be a better way to predict emotions than body movement would. \n",
    "<br><br>\n",
    "I think that if I were to take this project one step forward, I would collect a lot more data and use some machine learning and compare the accuracy results from that with this system's. I am interested in this idea of 'predicting' human behavior, but as we can see from some of these results and some of the discussions, human behavior is varying on a multitude of factors. I believe that the small amounts of data I had kept me from noticing a pattern in the values, not just what I observed myself in the videos. I also think that the idea of tuning to the truth versus general consensus was interesting because it shows how there are some things that are not easily explainable with equations and values for humans but are still predicted with high confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References: </h2>\n",
    "<br>\n",
    "\n",
    "Couceiro, M.S., Portugal, D., Gonçalves, N. et al. A methodology for detection and estimation in the analysis of golf putting. Pattern Anal Applic 16, 459–474 (2013). https://doi-org.ezproxy.cul.columbia.edu/10.1007/s10044-012-0276-8\n",
    "<br><br>\n",
    "Gong, W.; Zhang, X.; Gonzàlez, J.; Sobral, A.; Bouwmans, T.; Tu, C.; Zahzah, E.-H. Human Pose Estimation from Monocular Images: A Comprehensive Survey. Sensors 2016, 16, 1966. https://www.mdpi.com/1424-8220/16/12/1966\n",
    "<br><br>\n",
    "Noroozi, F., Kaminska, D., Corneanu, C., Sapinski, T., Escalera, S., & Anbarjafari, G. (2018). Survey on emotional body gesture recognition. IEEE transactions on affective computing. https://ieeexplore-ieee-org.ezproxy.cul.columbia.edu/stamp/stamp.jsp?tp=&arnumber=8493586&isnumber=5520654 \n",
    "<br><br>\n",
    "Park, S., Yong Chang, J., Jeong, H., Lee, J. H., & Park, J. Y. (2017). Accurate and efficient 3d human pose estimation algorithm using single depth images for pose analysis in golf. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 49-57). http://openaccess.thecvf.com/content_cvpr_2017_workshops/w2/papers/Park_Accurate_and_Efficient_CVPR_2017_paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
